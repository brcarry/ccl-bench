#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Traffic Interval Analyzer - é€šä¿¡è°ƒç”¨é—´éš”åˆ†æå™¨ (æŒ‰ Iteration åˆ‡ç‰‡)

æŒ‰æ¯ä¸ªè®­ç»ƒè¿­ä»£(iteration)åˆ†æ NCCL é›†åˆæ“ä½œçš„è°ƒç”¨é—´éš”å’ŒæŒç»­æ—¶é—´ï¼š
  - ä¸¤æ¬¡ AllGather ä¹‹é—´çš„æ—¶é—´é—´éš”åˆ†å¸ƒ
  - ä¸¤æ¬¡ ReduceScatter ä¹‹é—´çš„æ—¶é—´é—´éš”åˆ†å¸ƒ
  - ä¸¤æ¬¡ AllReduce ä¹‹é—´çš„æ—¶é—´é—´éš”åˆ†å¸ƒ
  - æ¯æ¬¡è°ƒç”¨çš„æŒç»­æ—¶é—´ç»Ÿè®¡

å…³é”®ç‰¹æ€§ï¼š
  - ä½¿ç”¨ NVTX marker è¯†åˆ« iteration è¾¹ç•Œ
  - åªç»Ÿè®¡ iteration å†…éƒ¨çš„é—´éš”ï¼Œæ’é™¤è·¨ iteration çš„é—´éš”
  - è¾“å‡º per-iteration å’Œæ±‡æ€»ç»Ÿè®¡

Usage:
    python traffic_interval_analyzer.py <nsys_directory_or_file>
    python traffic_interval_analyzer.py ../../nsys_1node
"""

import os
import re
import sqlite3
import subprocess
import numpy as np
from collections import defaultdict


# ============================================================================
# Iteration è¯†åˆ«ç›¸å…³
# ============================================================================

# æœ‰æ•ˆçš„ iteration marker æ¨¡å¼ï¼ˆå¸¦åºå·çš„ä¼˜å…ˆï¼‰
ITERATION_PATTERNS_WITH_NUM = [
    r'train\s+step\s+(\d+)',           # "Train Step 0", "train step 1"
    r'training\s+step\s+(\d+)',        # "training step 0"
    r'iteration\s+(\d+)',              # "iteration 0", "Iteration 1"
    r'^step\s+(\d+)$',                 # "step 0" (å®Œæ•´åŒ¹é…)
    r'^Train Step (\d+)$',             # ç²¾ç¡®åŒ¹é… "Train Step 0"
]

# æ²¡æœ‰åºå·çš„ iteration markerï¼ˆéœ€è¦æŒ‰å‡ºç°é¡ºåºç¼–å·ï¼‰
# æŒ‰ä¼˜å…ˆçº§æ’åˆ—ï¼šoptimizer.step ä¼˜äº forwardï¼ˆå› ä¸º forward å¯èƒ½è¢«å¤šæ¬¡è°ƒç”¨ï¼‰
ITERATION_PATTERNS_NO_NUM = [
    r'^DeepSpeedZeroOptimizer_Stage3\.step$',  # DeepSpeed ZeRO-3 optimizer step
    r'^DeepSpeedZeroOptimizer_Stage2\.step$',  # DeepSpeed ZeRO-2 optimizer step
    r'^optimizer\.step$',                       # Generic optimizer step
    r'^DeepSpeedEngine\.forward$',              # DeepSpeed forward (fallback for ZeRO-2)
]

# æ’é™¤çš„æ¨¡å¼
EXCLUDE_PATTERNS = [
    r'_post_step',
    r'backward_step',
    r'reduce_step',
    r'_step$',  # åªæ’é™¤ä»¥ _step ç»“å°¾çš„ï¼ˆä½†ä¸æ’é™¤ .stepï¼‰
]


def is_valid_iteration_marker(name):
    """
    æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ iteration marker
    
    Returns:
        tuple: (is_valid, iter_num_or_none, needs_numbering)
            - is_valid: æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ marker
            - iter_num_or_none: ä» marker ä¸­æå–çš„åºå·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None
            - needs_numbering: æ˜¯å¦éœ€è¦æŒ‰å‡ºç°é¡ºåºç¼–å·
    """
    # 1. å…ˆæ£€æŸ¥æ’é™¤æ¨¡å¼
    for pattern in EXCLUDE_PATTERNS:
        if re.search(pattern, name):
            return False, None, False
    
    # 2. æ£€æŸ¥å¸¦åºå·çš„æ¨¡å¼
    name_lower = name.lower()
    for pattern in ITERATION_PATTERNS_WITH_NUM:
        match = re.search(pattern, name_lower, re.IGNORECASE)
        if match:
            iter_num = int(match.group(1))
            return True, iter_num, False
    
    # 3. æ£€æŸ¥ä¸å¸¦åºå·çš„æ¨¡å¼ï¼ˆå¦‚ DeepSpeedZeroOptimizer_Stage3.stepï¼‰
    for pattern in ITERATION_PATTERNS_NO_NUM:
        if re.match(pattern, name):
            return True, None, True  # éœ€è¦æŒ‰å‡ºç°é¡ºåºç¼–å·
    
    return False, None, False


def get_iteration_boundaries(cursor):
    """
    ä» NVTX marker ä¸­è·å– iteration è¾¹ç•Œ
    
    ç­–ç•¥ï¼š
      1. ç›´æ¥æŸ¥è¯¢ NVTX_EVENTS è¡¨çš„ text å­—æ®µ
      2. ä¼˜å…ˆåŒ¹é…å¸¦åºå·çš„æ¨¡å¼ï¼ˆå¦‚ "Train Step 0"ï¼‰
      3. å¦‚æœæ²¡æœ‰å¸¦åºå·çš„ï¼Œä½¿ç”¨ä¸å¸¦åºå·çš„æ¨¡å¼ï¼ˆå¦‚ "DeepSpeedZeroOptimizer_Stage3.step"ï¼‰
      4. æ¯ä¸ª iteration å¯èƒ½æœ‰å¤šä¸ª GPU/thread çš„ markerï¼Œå–æ—¶é—´èŒƒå›´çš„å¹¶é›†
    
    Returns:
        list: [{'iter_num': int, 'start': ns, 'end': ns, 'name': str}, ...]
    """
    # ç›´æ¥æŸ¥è¯¢ NVTX_EVENTS è¡¨çš„ text å­—æ®µ
    try:
        query = """
        SELECT start, end, text, globalTid
        FROM NVTX_EVENTS
        WHERE text IS NOT NULL 
          AND start IS NOT NULL 
          AND end IS NOT NULL
        ORDER BY start
        """
        cursor.execute(query)
        ranges = cursor.fetchall()
    except sqlite3.OperationalError as e:
        print(f"  NVTX query failed: {e}")
        return []
    
    # æ”¶é›†ä¸¤ç§ç±»å‹çš„ marker
    numbered_markers = []   # å¸¦åºå·çš„ marker
    unnumbered_markers = [] # ä¸å¸¦åºå·çš„ marker
    
    for start, end, name, tid in ranges:
        if name is None:
            continue
        name_str = str(name)
        is_valid, iter_num, needs_numbering = is_valid_iteration_marker(name_str)
        
        if is_valid:
            if needs_numbering:
                unnumbered_markers.append({
                    'start': start,
                    'end': end,
                    'name': name_str,
                    'tid': tid
                })
            else:
                numbered_markers.append({
                    'iter_num': iter_num,
                    'start': start,
                    'end': end,
                    'name': name_str,
                    'tid': tid
                })
    
    # ä¼˜å…ˆä½¿ç”¨å¸¦åºå·çš„ marker
    if numbered_markers:
        # æŒ‰ iteration å·åˆ†ç»„
        iter_map = {}
        for m in numbered_markers:
            iter_num = m['iter_num']
            if iter_num not in iter_map:
                iter_map[iter_num] = {
                    'iter_num': iter_num,
                    'start': m['start'],
                    'end': m['end'],
                    'name': m['name']
                }
            else:
                # æ›´æ–°æ—¶é—´èŒƒå›´ï¼šå–æœ€æ—©å¼€å§‹å’Œæœ€æ™šç»“æŸ
                iter_map[iter_num]['start'] = min(iter_map[iter_num]['start'], m['start'])
                iter_map[iter_num]['end'] = max(iter_map[iter_num]['end'], m['end'])
        
        iterations = list(iter_map.values())
        iterations.sort(key=lambda x: x['iter_num'])
        return iterations
    
    # å¦‚æœæ²¡æœ‰å¸¦åºå·çš„ markerï¼Œä½¿ç”¨ä¸å¸¦åºå·çš„
    if unnumbered_markers:
        # æŒ‰ tid åˆ†ç»„ï¼Œæ¯ä¸ª tid çš„ marker æŒ‰å‡ºç°é¡ºåºç¼–å·
        # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ tid ä½œä¸ºä¸» tid
        from collections import Counter
        tid_counts = Counter(m['tid'] for m in unnumbered_markers)
        main_tid = tid_counts.most_common(1)[0][0]
        
        # åªä½¿ç”¨ä¸» tid çš„ marker
        main_markers = [m for m in unnumbered_markers if m['tid'] == main_tid]
        main_markers.sort(key=lambda x: x['start'])
        
        # ä½¿ç”¨ marker çš„ start åˆ°ä¸‹ä¸€ä¸ª marker çš„ start ä½œä¸º iteration èŒƒå›´
        # è¿™æ ·å¯ä»¥è¦†ç›– forward + backward + allreduce çš„å®Œæ•´å‘¨æœŸ
        iterations = []
        for i in range(len(main_markers)):
            start = main_markers[i]['start']
            # ç»“æŸæ—¶é—´ï¼šä¸‹ä¸€ä¸ª marker çš„å¼€å§‹æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ç”¨å½“å‰ marker çš„ç»“æŸæ—¶é—´
            if i < len(main_markers) - 1:
                end = main_markers[i + 1]['start'] - 1  # -1 é¿å…é‡å 
            else:
                end = main_markers[i]['end']
            
            iterations.append({
                'iter_num': i,
                'start': start,
                'end': end,
                'name': f"{main_markers[i]['name']} (auto #{i})"
            })
        
        return iterations
    
    return []


# ============================================================================
# NCCL æ“ä½œåˆ†ç±»
# ============================================================================

def get_collective_type(kernel_name):
    """ä» kernel åç§°ä¸­æå–é›†åˆæ“ä½œç±»å‹"""
    name_lower = kernel_name.lower()
    
    if 'allgather' in name_lower or 'all_gather' in name_lower:
        return 'AllGather'
    elif 'reducescatter' in name_lower or 'reduce_scatter' in name_lower:
        return 'ReduceScatter'
    elif 'allreduce' in name_lower or 'all_reduce' in name_lower:
        return 'AllReduce'
    elif 'broadcast' in name_lower:
        return 'Broadcast'
    elif 'alltoall' in name_lower or 'all_to_all' in name_lower:
        return 'AllToAll'
    elif 'send' in name_lower or 'recv' in name_lower:
        return 'SendRecv'
    else:
        return 'Other'


# ============================================================================
# ç»Ÿè®¡è®¡ç®—
# ============================================================================

def calc_stats(values_ms):
    """è®¡ç®—ç»Ÿè®¡é‡"""
    if not values_ms:
        return None
    
    arr = np.array(values_ms)
    return {
        'count': len(arr),
        'mean_ms': float(np.mean(arr)),
        'std_ms': float(np.std(arr)),
        'min_ms': float(np.min(arr)),
        'max_ms': float(np.max(arr)),
        'p50_ms': float(np.percentile(arr, 50)),
        'p95_ms': float(np.percentile(arr, 95)),
        'p99_ms': float(np.percentile(arr, 99))
    }


def calc_interval_stats(events):
    """
    è®¡ç®—äº‹ä»¶é—´éš”ç»Ÿè®¡
    
    Args:
        events: æŒ‰æ—¶é—´æ’åºçš„äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶æœ‰ start, end
    
    Returns:
        dict: é—´éš”ç»Ÿè®¡
    """
    if len(events) < 2:
        return None
    
    intervals_ns = []
    for i in range(len(events) - 1):
        # é—´éš” = ä¸‹ä¸€æ¬¡è°ƒç”¨çš„å¼€å§‹æ—¶é—´ - æœ¬æ¬¡è°ƒç”¨çš„ç»“æŸæ—¶é—´
        interval = events[i + 1]['start'] - events[i]['end']
        intervals_ns.append(interval)
    
    intervals_ms = [i / 1e6 for i in intervals_ns]
    
    # ç»Ÿè®¡æ­£é—´éš”å’Œè´Ÿé—´éš”
    positive = [i for i in intervals_ms if i > 0]
    negative = [i for i in intervals_ms if i < 0]
    
    stats = calc_stats(intervals_ms)
    if stats:
        stats['positive_count'] = len(positive)
        stats['negative_count'] = len(negative)
        stats['positive_mean_ms'] = float(np.mean(positive)) if positive else 0.0
        stats['negative_mean_ms'] = float(np.mean(negative)) if negative else 0.0
    
    return stats


# ============================================================================
# ä¸»åˆ†æå‡½æ•°
# ============================================================================

def analyze_traffic_intervals(nsys_rep_file):
    """
    æŒ‰ iteration åˆ‡ç‰‡åˆ†æ NCCL é€šä¿¡è°ƒç”¨çš„é—´éš”å’ŒæŒç»­æ—¶é—´
    
    Args:
        nsys_rep_file: .nsys-rep æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŒ…å« per-iteration å’Œæ±‡æ€»çš„åˆ†æç»“æœ
    """
    if not os.path.exists(nsys_rep_file):
        print(f"File not found: {nsys_rep_file}")
        return None
    
    print(f"Analyzing traffic intervals (per-iteration) in {nsys_rep_file}...")
    
    # 1. æ£€æŸ¥æˆ–å¯¼å‡º SQLite æ–‡ä»¶
    sqlite_file = nsys_rep_file.replace('.nsys-rep', '.sqlite')
    temp_sqlite = False
    
    if not os.path.exists(sqlite_file):
        sqlite_file = nsys_rep_file.replace('.nsys-rep', '_temp.sqlite')
        temp_sqlite = True
        
        print(f"  Exporting to SQLite...")
        try:
            export_cmd = ["nsys", "export", "--type=sqlite", f"--output={sqlite_file}",
                         "--force-overwrite=true", nsys_rep_file]
            subprocess.run(export_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=300)
            
            if not os.path.exists(sqlite_file):
                print(f"  Failed to export SQLite")
                return None
        except subprocess.TimeoutExpired:
            print(f"  Timeout exporting SQLite")
            return None
    
    try:
        conn = sqlite3.connect(sqlite_file)
        cursor = conn.cursor()
        
        # 2. è·å– iteration è¾¹ç•Œ
        iterations = get_iteration_boundaries(cursor)
        print(f"  Found {len(iterations)} iteration markers")
        
        if len(iterations) < 2:
            print("  Warning: Not enough iteration markers found, analyzing whole trace")
            # å›é€€åˆ°åˆ†ææ•´ä¸ª trace
            return analyze_whole_trace(cursor, conn, sqlite_file, temp_sqlite)
        
        # 3. æŸ¥è¯¢æ‰€æœ‰ NCCL kernel
        query = """
        SELECT k.start, k.end, s.value as name
        FROM CUPTI_ACTIVITY_KIND_KERNEL k
        JOIN StringIds s ON k.shortName = s.id
        WHERE s.value LIKE '%nccl%'
        ORDER BY k.start
        """
        
        cursor.execute(query)
        kernels = cursor.fetchall()
        conn.close()
        
        if temp_sqlite and os.path.exists(sqlite_file):
            os.remove(sqlite_file)
        
        if not kernels:
            print("  No NCCL kernels found")
            return None
        
        print(f"  Found {len(kernels)} NCCL events")
        
        # 4. å°† NCCL äº‹ä»¶åˆ†é…åˆ°å„ iteration
        #    ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–æ€§èƒ½
        iter_events = defaultdict(list)  # iter_num -> [events]
        
        for start, end, name in kernels:
            coll_type = get_collective_type(name)
            event = {
                'start': start,
                'end': end,
                'duration_ns': end - start,
                'type': coll_type
            }
            
            # æ‰¾åˆ°åŒ…å«è¿™ä¸ªäº‹ä»¶çš„ iteration
            # äº‹ä»¶çš„å¼€å§‹æ—¶é—´åœ¨ iteration çš„ [start, end] èŒƒå›´å†…
            for it in iterations:
                if it['start'] <= start <= it['end']:
                    iter_events[it['iter_num']].append(event)
                    break
        
        # 5. æŒ‰ iteration å’Œæ“ä½œç±»å‹åˆ†ç»„åˆ†æ
        per_iteration_stats = {}
        
        # ç”¨äºæ±‡æ€»çš„åˆ—è¡¨
        all_durations = defaultdict(list)  # coll_type -> [durations across all iters]
        all_intervals = defaultdict(list)  # coll_type -> [intervals across all iters]
        
        for iter_num in sorted(iter_events.keys()):
            events = iter_events[iter_num]
            
            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
            by_type = defaultdict(list)
            for e in events:
                by_type[e['type']].append(e)
            
            iter_stats = {
                'total_events': len(events),
                'per_operation': {}
            }
            
            for coll_type, type_events in by_type.items():
                # æŒ‰å¼€å§‹æ—¶é—´æ’åº
                type_events.sort(key=lambda x: x['start'])
                
                # æŒç»­æ—¶é—´ç»Ÿè®¡
                durations_ms = [e['duration_ns'] / 1e6 for e in type_events]
                duration_stats = calc_stats(durations_ms)
                
                if duration_stats:
                    duration_stats['total_time_ms'] = sum(durations_ms)
                
                # é—´éš”ç»Ÿè®¡ï¼ˆåªè®¡ç®— iteration å†…éƒ¨çš„é—´éš”ï¼‰
                interval_stats = calc_interval_stats(type_events)
                
                iter_stats['per_operation'][coll_type] = {
                    'duration': duration_stats,
                    'interval': interval_stats
                }
                
                # æ”¶é›†ç”¨äºæ±‡æ€»
                all_durations[coll_type].extend(durations_ms)
                if interval_stats and interval_stats['count'] > 0:
                    # æ”¶é›†æ¯ä¸ª iteration çš„é—´éš”å‡å€¼
                    all_intervals[coll_type].append(interval_stats['mean_ms'])
            
            per_iteration_stats[f"iter_{iter_num}"] = iter_stats
        
        # 6. è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary = {
            'num_iterations': len(per_iteration_stats),
            'total_nccl_events': len(kernels),
            'events_in_iterations': sum(
                s['total_events'] for s in per_iteration_stats.values()
            ),
            'per_operation': {}
        }
        
        for coll_type in all_durations.keys():
            durations = all_durations[coll_type]
            intervals = all_intervals[coll_type]
            
            summary['per_operation'][coll_type] = {
                'total_calls': len(durations),
                'calls_per_iteration': len(durations) / len(per_iteration_stats) if per_iteration_stats else 0,
                'duration': calc_stats(durations),
                'interval_mean_across_iters': {
                    'mean_ms': float(np.mean(intervals)) if intervals else None,
                    'std_ms': float(np.std(intervals)) if intervals else None,
                    'min_ms': float(np.min(intervals)) if intervals else None,
                    'max_ms': float(np.max(intervals)) if intervals else None,
                } if intervals else None
            }
            
            if summary['per_operation'][coll_type]['duration']:
                summary['per_operation'][coll_type]['duration']['total_time_ms'] = sum(durations)
        
        results = {
            'analysis_mode': 'per_iteration',
            'num_iterations': len(per_iteration_stats),
            'iteration_markers': [
                {'iter': it['iter_num'], 'name': it['name']} 
                for it in iterations[:5]  # åªä¿å­˜å‰5ä¸ªç¤ºä¾‹
            ],
            'summary': summary,
            'per_iteration': per_iteration_stats
        }
        
        return results
        
    except Exception as e:
        print(f"  Error: {e}")
        import traceback
        traceback.print_exc()
        if temp_sqlite and os.path.exists(sqlite_file):
            os.remove(sqlite_file)
        return None


def analyze_whole_trace(cursor, conn, sqlite_file, temp_sqlite):
    """
    åˆ†ææ•´ä¸ª traceï¼ˆå½“æ²¡æœ‰è¶³å¤Ÿçš„ iteration marker æ—¶çš„å›é€€æ–¹æ¡ˆï¼‰
    """
    print("  Falling back to whole-trace analysis...")
    
    query = """
    SELECT k.start, k.end, s.value as name
    FROM CUPTI_ACTIVITY_KIND_KERNEL k
    JOIN StringIds s ON k.shortName = s.id
    WHERE s.value LIKE '%nccl%'
    ORDER BY k.start
    """
    
    cursor.execute(query)
    kernels = cursor.fetchall()
    conn.close()
    
    if temp_sqlite and os.path.exists(sqlite_file):
        os.remove(sqlite_file)
    
    if not kernels:
        return None
    
    # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
    by_type = defaultdict(list)
    
    for start, end, name in kernels:
        coll_type = get_collective_type(name)
        by_type[coll_type].append({
            'start': start,
            'end': end,
            'duration_ns': end - start
        })
    
    results = {
        'analysis_mode': 'whole_trace',
        'total_nccl_events': len(kernels),
        'per_operation': {}
    }
    
    for coll_type, events in by_type.items():
        events.sort(key=lambda x: x['start'])
        
        durations_ms = [e['duration_ns'] / 1e6 for e in events]
        duration_stats = calc_stats(durations_ms)
        if duration_stats:
            duration_stats['total_time_ms'] = sum(durations_ms)
        
        interval_stats = calc_interval_stats(events)
        
        results['per_operation'][coll_type] = {
            'duration': duration_stats,
            'interval': interval_stats
        }
    
    return results


def analyze_directory(nsys_path):
    """åˆ†æç›®å½•æˆ–å•ä¸ªæ–‡ä»¶"""
    nsys_files = []
    
    if os.path.isfile(nsys_path):
        if nsys_path.endswith('.nsys-rep'):
            nsys_files.append(nsys_path)
    elif os.path.isdir(nsys_path):
        for f in os.listdir(nsys_path):
            if f.endswith('.nsys-rep'):
                nsys_files.append(os.path.join(nsys_path, f))
    
    if not nsys_files:
        print(f"No .nsys-rep files found")
        return None
    
    print(f"Found {len(nsys_files)} nsys-rep file(s)\n")
    
    all_results = []
    for nsys_file in sorted(nsys_files):
        result = analyze_traffic_intervals(nsys_file)
        if result:
            result['file'] = os.path.basename(nsys_file)
            all_results.append(result)
    
    if len(all_results) == 1:
        return all_results[0]
    
    # å¤šæ–‡ä»¶æ—¶è¿”å›ç¬¬ä¸€ä¸ªçš„è¯¦ç»†ç»“æœ
    merged = all_results[0].copy()
    merged['num_files'] = len(all_results)
    return merged


def print_results(results):
    """æ‰“å°åˆ†æç»“æœ"""
    print("\n" + "=" * 80)
    print("Traffic Interval Analysis - é€šä¿¡è°ƒç”¨é—´éš”åˆ†æ (Per-Iteration)")
    print("=" * 80)
    
    mode = results.get('analysis_mode', 'unknown')
    print(f"Analysis mode: {mode}")
    
    if mode == 'per_iteration':
        print(f"Number of iterations: {results['num_iterations']}")
        
        if results.get('iteration_markers'):
            print(f"Iteration markers (sample): {[m['name'] for m in results['iteration_markers'][:3]]}")
        
        summary = results.get('summary', {})
        print(f"Total NCCL events: {summary.get('total_nccl_events', 'N/A')}")
        print(f"Events in iterations: {summary.get('events_in_iterations', 'N/A')}")
        
        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        print(f"\n{'â”€' * 80}")
        print("ğŸ“Š Summary Statistics (across all iterations)")
        print(f"{'â”€' * 80}")
        
        for coll_type, data in sorted(summary.get('per_operation', {}).items(),
                                       key=lambda x: x[1].get('total_calls', 0),
                                       reverse=True):
            print(f"\n  {coll_type}:")
            print(f"    Total calls: {data.get('total_calls', 'N/A')}")
            print(f"    Calls per iteration: {data.get('calls_per_iteration', 0):.1f}")
            
            dur = data.get('duration')
            if dur:
                print(f"    Duration - Mean: {dur['mean_ms']:.4f} ms, P50: {dur['p50_ms']:.4f} ms, P99: {dur['p99_ms']:.4f} ms")
            
            intv = data.get('interval_mean_across_iters')
            if intv and intv.get('mean_ms') is not None:
                print(f"    Interval (mean across iters) - Mean: {intv['mean_ms']:.4f} ms, Std: {intv['std_ms']:.4f} ms")
        
        # æ‰“å°ç¤ºä¾‹ iteration
        per_iter = results.get('per_iteration', {})
        if per_iter:
            sample_iter = list(per_iter.keys())[0]
            sample_data = per_iter[sample_iter]
            
            print(f"\n{'â”€' * 80}")
            print(f"ğŸ“Š Sample Iteration: {sample_iter}")
            print(f"{'â”€' * 80}")
            print(f"  Total events: {sample_data.get('total_events', 'N/A')}")
            
            for coll_type, op_data in sample_data.get('per_operation', {}).items():
                dur = op_data.get('duration')
                intv = op_data.get('interval')
                
                if dur:
                    print(f"\n  {coll_type}:")
                    print(f"    Calls: {dur['count']}")
                    print(f"    Duration - Mean: {dur['mean_ms']:.4f} ms, Total: {dur.get('total_time_ms', 0):.2f} ms")
                    
                    if intv:
                        print(f"    Interval - Mean: {intv['mean_ms']:.4f} ms, Pos: {intv['positive_count']}, Neg: {intv['negative_count']}")
    
    else:
        # whole_trace mode
        print(f"Total NCCL events: {results.get('total_nccl_events', 'N/A')}")
        
        for coll_type, data in results.get('per_operation', {}).items():
            dur = data.get('duration')
            intv = data.get('interval')
            
            print(f"\n  {coll_type}:")
            if dur:
                print(f"    Calls: {dur['count']}, Total: {dur.get('total_time_ms', 0):.2f} ms")
                print(f"    Duration - Mean: {dur['mean_ms']:.4f} ms, P99: {dur['p99_ms']:.4f} ms")
            if intv:
                print(f"    Interval - Mean: {intv['mean_ms']:.4f} ms")
    
    print("\n" + "=" * 80)


def metric_cal(directory):
    """å…¼å®¹ CCL-Bench æ¥å£"""
    results = analyze_directory(directory)
    if results:
        print_results(results)
    return results


if __name__ == "__main__":
    import sys
    import json
    
    if len(sys.argv) > 1:
        path = sys.argv[1]
        results = metric_cal(path)
        
        if len(sys.argv) > 2:
            output_file = sys.argv[2]
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print("Usage: python traffic_interval_analyzer.py <nsys_directory_or_file> [output.json]")
