# workload_template.yaml
version: 1

description: >
  Llama3 8B training on 16 GPUs with TP=4, PP=2, DP(shard)=2. Torch distributed
  runtime, NCCL comms over Slingshot/RoCEv2. No compile/fusion; selective
  activation checkpointing.

trace_url: https://drive.google.com/file/d/1EK_YROAho2sBvYVn8idF6DbnaG3SZ0he/view

workload_definition:
  phase: training 
  moe: false
  granularity: model_fwd_bwd_pass
  model_family: lama-3.1-8b
  local_batch_size: 4
  seq_len: 8192
  dataset: c4
  floating_point_precision: bf16 #TODO verify

framework: torchtitan

communication_library: 
  name: NCCL
  env:
    NCCL_IB_QPS_PER_CONNECTION: "4" # TODO verify, add more envs as needed

protocol_selection:
  - rocev2 #TODO verify
  - p2p
compiler_tool_selection: plain_pytorch
network_topology_bandwidth_latency:
  topology: slingshot
  bandwidth_gbps:
    - 200              # TODO adjust scale-out
    - 2000             # TODO adjust scale-up

xpu_specification:
  type: GPU
  model: "nvidia_a100"
  total_count: 16
  count_per_node: 4
  
model_plan_parallelization:
  dp_replicate: 1
  dp_shard: 2
  tp: 4
  pp: 2
  cp: 1

tracing:
  traces:
    - nsys
    - torch_et
    - kineto_trace
  metrics_specific_trace:
